{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 5386 - NATURAL LANGUAGE PROCESSING\n",
    "## Final Project\n",
    "Name: Sandeep Kumar Reddy Kadapa <br>\n",
    "Student ID: 300154284 <br>\n",
    "Email ID: skada089@uottawa.ca\n",
    "\n",
    "Name: Geetika Sharma <br>\n",
    "Student ID: 100993465 <br>\n",
    "Email: gshar013@uottawa.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuters-21578 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      The Reuters-21578 benchmark corpus, ApteMod version\n",
      "\n",
      "This is a publically available version of the well-known Reuters-21578\n",
      "\"ApteMod\" corpus for text categorization.  It has been used in\n",
      "publications like these:\n",
      "\n",
      " * Yiming Yang and X. Liu. \"A re-examination of text categorization\n",
      "   methods\".  1999.  Proceedings of 22nd Annual International SIGIR.\n",
      "   http://citeseer.nj.nec.com/yang99reexamination.html\n",
      "\n",
      " * Thorsten Joachims. \"Text categorization with support vector\n",
      "   machines: learning with many relevant features\".  1998. Proceedings\n",
      "   of ECML-98, 10th European Conference on Machine Learning.\n",
      "   http://citeseer.nj.nec.com/joachims98text.html\n",
      "\n",
      "ApteMod is a collection of 10,788 documents from the Reuters financial\n",
      "newswire service, partitioned into a training set with 7769 documents\n",
      "and a test set with 3019 documents.  The total size of the corpus is\n",
      "about 43 MB.  It is also available for download from\n",
      "http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.html ,\n",
      "which includes a more extensive history of the data revisions.\n",
      "\n",
      "The distribution of categories in the ApteMod corpus is highly skewed,\n",
      "with 36.7% of the documents in the most common category, and only\n",
      "0.0185% (2 documents) in each of the five least common categories.\n",
      "In fact, the original data source is even more skewed---in creating\n",
      "the corpus, any categories that did not contain at least one document\n",
      "in the training set and one document in the test set were removed from\n",
      "the corpus by its original creator.\n",
      "\n",
      "In the ApteMod corpus, each document belongs to one or more\n",
      "categories.  There are 90 categories in the corpus.  The average\n",
      "number of categories per document is 1.235, and the average number of\n",
      "documents per category is about 148, or 1.37% of the corpus.\n",
      "\n",
      " -Ken Williams\n",
      "  ken@mathforum.org\n",
      "\n",
      "         Copyright & Notification \n",
      "\n",
      "(extracted from the README at the UCI address above)\n",
      "\n",
      "The copyright for the text of newswire articles and Reuters\n",
      "annotations in the Reuters-21578 collection resides with Reuters Ltd.\n",
      "Reuters Ltd. and Carnegie Group, Inc. have agreed to allow the free\n",
      "distribution of this data *for research purposes only*.  \n",
      "\n",
      "If you publish results based on this data set, please acknowledge\n",
      "its use, refer to the data set by the name \"Reuters-21578,\n",
      "Distribution 1.0\", and inform your readers of the current location of\n",
      "the data set (see \"Availability & Questions\").\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reuters.readme())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting training and testing data from nltk\n",
    "train_documents, train_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('training/')])\n",
    "test_documents, test_categories = zip(*[(reuters.raw(i), reuters.categories(i)) for i in reuters.fileids() if i.startswith('test/')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents: 7769\n",
      "Number of testing documents: 3019\n"
     ]
    }
   ],
   "source": [
    "#number of documents of training and testing\n",
    "print(\"Number of training documents:\", len(train_documents))\n",
    "print(\"Number of testing documents:\", len(test_documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Reuters dataset\n",
    "1. we removed spaces, digits, punctuation and special characters and considering only text data\n",
    "2. Calculated the top categories and considered them for training and testing data\n",
    "3. Preprocess categories since some of the categories are multilabel\n",
    "4. Remove stop words from the data\n",
    "5. Finally creating and saving word vectors for our new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing spaces, digits, punctuation and special characters, considering only text data\n",
    "train_documents = [re.sub('\\\\/|\\s+', ' ', re.sub('!|\\\\\"|\\\\#|\\\\$|\\\\%|\\\\&|\\\\(|\\\\)|\\\\*|\\\\+|\\\\,|\\\\-|\\\\.|\\\\:|\\\\;|\\\\<|\\\\=|\\\\>|\\\\?|\\\\@|\\\\[|\\\\\\\\|\\\\]|\\\\^|\\\\_|\\\\`|\\\\{|\\\\||\\\\}|\\\\~|\\\\\\t|\\\\\\n|\\'|\\,|[0-9]', '', doc.lower())).strip() for doc in train_documents]\n",
    "test_documents = [re.sub('\\\\/|\\s+', ' ', re.sub('!|\\\\\"|\\\\#|\\\\$|\\\\%|\\\\&|\\\\(|\\\\)|\\\\*|\\\\+|\\\\,|\\\\-|\\\\.|\\\\:|\\\\;|\\\\<|\\\\=|\\\\>|\\\\?|\\\\@|\\\\[|\\\\\\\\|\\\\]|\\\\^|\\\\_|\\\\`|\\\\{|\\\\||\\\\}|\\\\~|\\\\\\t|\\\\\\n|\\'|\\,|[0-9]', '', doc.lower())).strip() for doc in test_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "earn        3964\n",
       "acq         2369\n",
       "money-fx     717\n",
       "grain        582\n",
       "crude        578\n",
       "trade        485\n",
       "interest     478\n",
       "ship         286\n",
       "wheat        283\n",
       "corn         237\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the count of top categories containing highest samples in the dataset\n",
    "from itertools import chain\n",
    "pd.Series(list(chain.from_iterable(train_categories))+list(chain.from_iterable(test_categories))).value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only considering top 10 categories containing high samples\n",
    "categories = ['earn', 'acq', 'money-fx', 'grain', 'crude', 'trade', 'interest', 'ship', 'wheat', 'corn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess the categories, because some multi categories in those situations we are considering only the first category\n",
    "def preprocess_categories(categories, data_categories):\n",
    "    #creating empty lists to store the categories and their index\n",
    "    categories_new = []\n",
    "    categories_index = []\n",
    "    \n",
    "    #looping thorugh the categories\n",
    "    for cat in data_categories:\n",
    "        #initially setting the index and category\n",
    "        index=False\n",
    "        category=None\n",
    "        #iterating through the sub categories\n",
    "        for cat2 in cat[::-1]:\n",
    "            #if the category is in the list of categories store the category and make the index=True\n",
    "            if cat2 in categories:\n",
    "                category=cat2\n",
    "                index=True\n",
    "            if not category:\n",
    "                category=None\n",
    "        #finally append the category and whether its index for considering\n",
    "        categories_index.append(index)\n",
    "        categories_new.append(category)\n",
    "    return categories_index, categories_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from above we get the index to consider and categories\n",
    "train_categories_index, train_categories_new = preprocess_categories(categories, train_categories)\n",
    "test_categories_index, test_categories_new = preprocess_categories(categories, test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the documents and categories where index is True by filters like below\n",
    "train_documents = np.array(train_documents, dtype='object')[train_categories_index]\n",
    "train_categories = np.array(train_categories_new)[train_categories_index]\n",
    "test_documents = np.array(test_documents, dtype='object')[test_categories_index]\n",
    "test_categories = np.array(test_categories_new)[test_categories_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training documents: 6489\n",
      "Number of testing documents: 2545\n"
     ]
    }
   ],
   "source": [
    "#Now print the number of documents in the new dataset for top 10 categories\n",
    "print(\"Number of training documents:\", len(train_documents))\n",
    "print(\"Number of testing documents:\", len(test_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove stopwords\n",
    "stop_words_list = stopwords.words('english')\n",
    "def tokenize_remove_stopwords(data):\n",
    "    # create a empty list\n",
    "    data_new = []\n",
    "    for doc in data:\n",
    "        # iterate through the documents\n",
    "        doc_new = []\n",
    "        # tokenize the docuemnt\n",
    "        tokenized_words = word_tokenize(doc)\n",
    "        for word in tokenized_words:\n",
    "            #check whether the word is in stop words if not consider it\n",
    "            if not word in stop_words_list:\n",
    "                doc_new.append(word)\n",
    "        data_new.append(doc_new)\n",
    "    #finally return the data that contains no stop words\n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the stopwords from the training and testing data\n",
    "train_documents = tokenize_remove_stopwords(train_documents)\n",
    "test_documents = tokenize_remove_stopwords(test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word Vectors\n",
    "We considered the following embeddings for our data\n",
    "1. Glove_6B_300d\n",
    "2. Glove_840B_300d\n",
    "3. Google_News\n",
    "4. FastText_SkipGram\n",
    "5. FastText_CBOW\n",
    "6. LexVec_CC_300d\n",
    "7. LexVec_WN_300d\n",
    "8. PDC\n",
    "9. HDC\n",
    "10. ConceptNet_Numberbatch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the genism functions for creating word vectors \n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove word embeddings cannot be used directly so we need to first save them in word to vector format so we are using below function to do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_input_file = './Glove 840B 300d/glove.840B.300d.txt'\n",
    "#word2vec_output_file = './Glove 840B 300d/glove.840B.300d.txt.word2vec'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#glove_input_file = './Glove 6B 300d/glove.6B.300d.txt'\n",
    "#word2vec_output_file = './Glove 6B 300d/glove.6B.300d.txt.word2vec'\n",
    "#glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings path and thier names for further analysis and accessing\n",
    "embedding_list_path = ['./Glove 6B 300d/glove.6B.300d.txt.word2vec', './Glove 840B 300d/glove.840B.300d.txt.word2vec', \n",
    "                       './Google News 300d/GoogleNews-vectors-negative300.bin', \n",
    "                       './FastText SkipGram//wiki.en.vec', './FastText CBOW/cc.en.300.vec', \n",
    "                       './LexVec CC 300d/lexvec.commoncrawl.300d.W+C.pos.neg3.vectors', './LexVec WN 300d/lexvec.enwiki+newscrawl.300d.W+C.pos.vectors', \n",
    "                       './PDC/wikicorp.201004-pdc-iter-20-alpha-0.05-window-10-dim-300-neg-10-subsample-0.0001.txt', \n",
    "                       './HDC/wikicorp.201004-hdc-iter-20-alpha-0.025-window-10-dim-300-neg-10-subsample-0.0001.txt',\n",
    "                       './ConceptNet Numberbatch/numberbatch-en.txt']\n",
    "\n",
    "embedding_names = ['Glove_6B_300d', 'Glove_840B_300d', 'Google_News', 'FastText_SkipGram', 'FastText_CBOW', 'LexVec_CC_300d', 'LexVec_WN_300d', 'PDC', 'HDC', 'ConceptNet_Numberbatch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_word2vec_embeddings will save the embeddings that are transformed in given path\n",
    "def create_word2vec_embeddings(embeddings, data, index, name):\n",
    "    if name=='test':\n",
    "        print('Create word vectors for testing data')\n",
    "    else:\n",
    "        print('Creating word vectors for training data')\n",
    "    data_list_mean = []\n",
    "    data_list_sum = []\n",
    "    for doc in data:\n",
    "        doc_list = []\n",
    "        for word in doc:\n",
    "            try:\n",
    "                vector = embeddings[word]\n",
    "                doc_list.append(vector)\n",
    "            except:\n",
    "                pass\n",
    "        mean_vec = np.stack(doc_list, axis=1).mean(axis=1)\n",
    "        sum_vec = np.stack(doc_list, axis=1).sum(axis=1)\n",
    "        data_list_mean.append(mean_vec)\n",
    "        data_list_sum.append(sum_vec)\n",
    "    print(np.array(data_list_mean).shape)\n",
    "    print(np.array(data_list_sum).shape)\n",
    "    np.save(f'./embeddings/{name}_mean_'+embedding_names[index],np.array(data_list_mean))\n",
    "    np.save(f'./embeddings/{name}_sum_'+embedding_names[index],np.array(data_list_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_embeddings function will convert the tokenized words into their corresponding word vectors of given embedding data and also we will do both sum and mean of the entire document data\n",
    "#since concating will give us 3 dimensional array and our machine learning algorithms won't work with 3 dimensional data.\n",
    "\n",
    "def get_embeddings(train_docs, test_docs):\n",
    "    #iterate through each embedding\n",
    "    for index, path in enumerate(embedding_list_path):\n",
    "        print(f'Loading embedding from {path}')\n",
    "        \n",
    "        if not 'Google' in path:\n",
    "            embeddings = KeyedVectors.load_word2vec_format(path)\n",
    "        else:\n",
    "            embeddings = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "        print('Loading complete')\n",
    "        \n",
    "        create_word2vec_embeddings(embeddings, train_docs, index, 'train')\n",
    "        create_word2vec_embeddings(embeddings, test_docs, index, 'test')\n",
    "        \n",
    "        print(f'Completed Word vectors for {path} embedding','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from ./Glove 6B 300d/glove.6B.300d.txt.word2vec\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./Glove 6B 300d/glove.6B.300d.txt.word2vec embedding \n",
      "\n",
      "Loading embedding from ./Glove 840B 300d/glove.840B.300d.txt.word2vec\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./Glove 840B 300d/glove.840B.300d.txt.word2vec embedding \n",
      "\n",
      "Loading embedding from ./Google News 300d/GoogleNews-vectors-negative300.bin\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./Google News 300d/GoogleNews-vectors-negative300.bin embedding \n",
      "\n",
      "Loading embedding from ./FastText SkipGram//wiki.en.vec\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./FastText SkipGram//wiki.en.vec embedding \n",
      "\n",
      "Loading embedding from ./FastText CBOW/cc.en.300.vec\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./FastText CBOW/cc.en.300.vec embedding \n",
      "\n",
      "Loading embedding from ./LexVec CC 300d/lexvec.commoncrawl.300d.W+C.pos.neg3.vectors\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./LexVec CC 300d/lexvec.commoncrawl.300d.W+C.pos.neg3.vectors embedding \n",
      "\n",
      "Loading embedding from ./LexVec WN 300d/lexvec.enwiki+newscrawl.300d.W+C.pos.vectors\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./LexVec WN 300d/lexvec.enwiki+newscrawl.300d.W+C.pos.vectors embedding \n",
      "\n",
      "Loading embedding from ./PDC/wikicorp.201004-pdc-iter-20-alpha-0.05-window-10-dim-300-neg-10-subsample-0.0001.txt\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./PDC/wikicorp.201004-pdc-iter-20-alpha-0.05-window-10-dim-300-neg-10-subsample-0.0001.txt embedding \n",
      "\n",
      "Loading embedding from ./HDC/wikicorp.201004-hdc-iter-20-alpha-0.025-window-10-dim-300-neg-10-subsample-0.0001.txt\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./HDC/wikicorp.201004-hdc-iter-20-alpha-0.025-window-10-dim-300-neg-10-subsample-0.0001.txt embedding \n",
      "\n",
      "Loading embedding from ./ConceptNet Numberbatch/numberbatch-en.txt\n",
      "Loading complete\n",
      "Creating word vectors for training data\n",
      "(6489, 300)\n",
      "(6489, 300)\n",
      "Create word vectors for testing data\n",
      "(2545, 300)\n",
      "(2545, 300)\n",
      "Completed Word vectors for ./ConceptNet Numberbatch/numberbatch-en.txt embedding \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get_embeddings(train_documents, test_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all necessary functions from s\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier\n",
    "\n",
    "#inherently multiclass\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# One-Vs-One\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "#One-Vs-Rest\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary of algorithms we test\n",
    "def create_model_dict():\n",
    "\n",
    "    models = {'GaussianNB':GaussianNB(), \n",
    "              'BernoulliNB':BernoulliNB(), \n",
    "              'DecisionTree':DecisionTreeClassifier(), \n",
    "              'KNeighbors':KNeighborsClassifier(), \n",
    "              'MLPClassifier':MLPClassifier(hidden_layer_sizes=(100, 50, 10)), \n",
    "              'RandomForest':RandomForestClassifier(), \n",
    "              'ExtraTrees':ExtraTreesClassifier(), \n",
    "              'SVC':SVC(),\n",
    "              'LogisticRegression':LogisticRegression()}\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the categorical values into numerical values using LabelEncoder from sklearn\n",
    "LE = LabelEncoder()\n",
    "y_train = LE.fit_transform(train_categories)\n",
    "y_test = LE.transform(test_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data function will read and return the training and testing data of sum and mean of vectors we saved above\n",
    "def get_data(name):\n",
    "    #creating names of the files\n",
    "    train_sum_file_name = './embeddings/train_sum_'+name+'.npy'\n",
    "    train_mean_file_name = './embeddings/train_mean_'+name+'.npy'\n",
    "    test_sum_file_name = './embeddings/test_sum_'+name+'.npy'\n",
    "    test_mean_file_name = './embeddings/test_mean_'+name+'.npy'\n",
    "    #loading saved numpy vectors\n",
    "    X_train_sum = np.load(train_sum_file_name)\n",
    "    X_train_mean = np.load(train_mean_file_name)\n",
    "    X_test_sum = np.load(test_sum_file_name)\n",
    "    X_test_mean = np.load(test_mean_file_name)\n",
    "    return X_train_sum, X_train_mean, X_test_sum, X_test_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_model_scores will calculate the performance metrics of the model for each algorithm\n",
    "def get_model_scores(X_train, y_train, X_test, y_test):\n",
    "    #creating empty list to store the metric values\n",
    "    all_metrics=[]\n",
    "    #creating dictionary of algorithms for analysis\n",
    "    models = create_model_dict()\n",
    "    #calculating the performance for each model\n",
    "    for model_name, model in models.items():\n",
    "        #since not all algorithms cannot be run directly using same method so created appropriate wrapper function\n",
    "        print(f'Runnning {model_name} algorithm')\n",
    "        metrics = []\n",
    "        if not model_name in ['SVC', 'GradientBoosting', 'LogisticRegression']:\n",
    "            classifier = model\n",
    "        elif model_name == 'SVC':\n",
    "            classifier = OneVsOneClassifier(model)\n",
    "        else:\n",
    "            classifier = OneVsRestClassifier(model)\n",
    "            \n",
    "        classifier.fit(X_train, y_train)\n",
    "        #calculating the accuracy, precision, f1_score, recall\n",
    "        metrics.append(accuracy_score(classifier.predict(X_test), y_test))\n",
    "        metrics.append(precision_score(classifier.predict(X_test), y_test, average='macro'))\n",
    "        metrics.append(f1_score(classifier.predict(X_test), y_test, average='macro'))\n",
    "        metrics.append(recall_score(classifier.predict(X_test), y_test, average='macro'))\n",
    "        all_metrics.append(metrics)\n",
    "        \n",
    "    return all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runnning Glove_6B_300d embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Glove_6B_300d completed \n",
      "\n",
      "Runnning Glove_840B_300d embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Glove_840B_300d completed \n",
      "\n",
      "Runnning Google_News embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Google_News completed \n",
      "\n",
      "Runnning FastText_SkipGram embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "FastText_SkipGram completed \n",
      "\n",
      "Runnning FastText_CBOW embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "FastText_CBOW completed \n",
      "\n",
      "Runnning LexVec_CC_300d embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "LexVec_CC_300d completed \n",
      "\n",
      "Runnning LexVec_WN_300d embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "LexVec_WN_300d completed \n",
      "\n",
      "Runnning PDC embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "PDC completed \n",
      "\n",
      "Runnning HDC embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "HDC completed \n",
      "\n",
      "Runnning ConceptNet_Numberbatch embedding\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "Runnning GaussianNB algorithm\n",
      "Runnning BernoulliNB algorithm\n",
      "Runnning DecisionTree algorithm\n",
      "Runnning KNeighbors algorithm\n",
      "Runnning MLPClassifier algorithm\n",
      "Runnning RandomForest algorithm\n",
      "Runnning ExtraTrees algorithm\n",
      "Runnning SVC algorithm\n",
      "Runnning LogisticRegression algorithm\n",
      "ConceptNet_Numberbatch completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#calculating the metrics for all combinations of Algorithms as well as embeddings\n",
    "final_sum_models = []\n",
    "final_mean_models = []\n",
    "for embedding_name in embedding_names:\n",
    "    print(f'Runnning {embedding_name} embedding')\n",
    "    X_train_sum, X_train_mean, X_test_sum, X_test_mean = get_data(embedding_name)\n",
    "    final_sum_models.append(get_model_scores(X_train_sum, y_train, X_test_sum, y_test))\n",
    "    final_mean_models.append(get_model_scores(X_train_mean, y_train, X_test_mean, y_test))\n",
    "    print(f'{embedding_name} completed', '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_models = pd.concat([pd.DataFrame(np.array(final_sum_models)[i, :, :], columns=['Accuracy', 'Precision', 'F1_score', 'Recall'],\n",
    "                                        index=list(create_model_dict().keys()))\\\n",
    "                           .reset_index().rename(columns={'index':'Algorithm'})\\\n",
    "                           .merge(pd.Series([embedding_names[i]]*9, name='Embedding'), \n",
    "                                  left_index=True, right_index=True) for i in range(10)], \n",
    "                          ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.640472</td>\n",
       "      <td>0.425184</td>\n",
       "      <td>0.422551</td>\n",
       "      <td>0.518910</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.866012</td>\n",
       "      <td>0.682701</td>\n",
       "      <td>0.648979</td>\n",
       "      <td>0.651998</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.790570</td>\n",
       "      <td>0.526497</td>\n",
       "      <td>0.528317</td>\n",
       "      <td>0.537312</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.899411</td>\n",
       "      <td>0.751334</td>\n",
       "      <td>0.759949</td>\n",
       "      <td>0.774177</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.922200</td>\n",
       "      <td>0.799785</td>\n",
       "      <td>0.805708</td>\n",
       "      <td>0.832047</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.910806</td>\n",
       "      <td>0.760101</td>\n",
       "      <td>0.778731</td>\n",
       "      <td>0.826032</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.905305</td>\n",
       "      <td>0.741856</td>\n",
       "      <td>0.767746</td>\n",
       "      <td>0.833739</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.920629</td>\n",
       "      <td>0.767278</td>\n",
       "      <td>0.781157</td>\n",
       "      <td>0.819783</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.921022</td>\n",
       "      <td>0.822466</td>\n",
       "      <td>0.821033</td>\n",
       "      <td>0.823424</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Algorithm  Accuracy  Precision  F1_score    Recall      Embedding\n",
       "0          GaussianNB  0.640472   0.425184  0.422551  0.518910  Glove_6B_300d\n",
       "1         BernoulliNB  0.866012   0.682701  0.648979  0.651998  Glove_6B_300d\n",
       "2        DecisionTree  0.790570   0.526497  0.528317  0.537312  Glove_6B_300d\n",
       "3          KNeighbors  0.899411   0.751334  0.759949  0.774177  Glove_6B_300d\n",
       "4       MLPClassifier  0.922200   0.799785  0.805708  0.832047  Glove_6B_300d\n",
       "5        RandomForest  0.910806   0.760101  0.778731  0.826032  Glove_6B_300d\n",
       "6          ExtraTrees  0.905305   0.741856  0.767746  0.833739  Glove_6B_300d\n",
       "7                 SVC  0.920629   0.767278  0.781157  0.819783  Glove_6B_300d\n",
       "8  LogisticRegression  0.921022   0.822466  0.821033  0.823424  Glove_6B_300d"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum_models.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_models['Mean_score'] = df_sum_models.mean(axis=1)\n",
    "df_sum_models['Rank'] = df_sum_models['Mean_score'].rank(ascending=False)\n",
    "df_sum_models = df_sum_models.sort_values(by='Rank').reset_index(drop=True)\n",
    "df_sum_models = df_sum_models.loc[:, ['Embedding', 'Algorithm','Accuracy', 'Precision', 'F1_score', 'Recall', 'Mean_score', 'Rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sum_models.to_csv('models_sum.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_models = pd.concat([pd.DataFrame(np.array(final_mean_models)[i, :, :], columns=['Accuracy', 'Precision', 'F1_score', 'Recall'], \n",
    "                                         index=list(create_model_dict().keys()))\\\n",
    "                            .reset_index().rename(columns={'index':'Algorithm'})\\\n",
    "                            .merge(pd.Series([embedding_names[i]]*9, name='Embedding'), \n",
    "                                   left_index=True, right_index=True) for i in range(10)], \n",
    "                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1_score</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.862083</td>\n",
       "      <td>0.758189</td>\n",
       "      <td>0.706449</td>\n",
       "      <td>0.705963</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BernoulliNB</td>\n",
       "      <td>0.866012</td>\n",
       "      <td>0.682701</td>\n",
       "      <td>0.648979</td>\n",
       "      <td>0.651998</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree</td>\n",
       "      <td>0.808251</td>\n",
       "      <td>0.609814</td>\n",
       "      <td>0.606901</td>\n",
       "      <td>0.607175</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>0.915128</td>\n",
       "      <td>0.770038</td>\n",
       "      <td>0.779840</td>\n",
       "      <td>0.807331</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.926523</td>\n",
       "      <td>0.819634</td>\n",
       "      <td>0.825720</td>\n",
       "      <td>0.846252</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.917878</td>\n",
       "      <td>0.777735</td>\n",
       "      <td>0.796195</td>\n",
       "      <td>0.838551</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>0.916306</td>\n",
       "      <td>0.774920</td>\n",
       "      <td>0.794646</td>\n",
       "      <td>0.835383</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.939489</td>\n",
       "      <td>0.842915</td>\n",
       "      <td>0.854771</td>\n",
       "      <td>0.878358</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.936346</td>\n",
       "      <td>0.839541</td>\n",
       "      <td>0.851763</td>\n",
       "      <td>0.877172</td>\n",
       "      <td>Glove_6B_300d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Algorithm  Accuracy  Precision  F1_score    Recall      Embedding\n",
       "0          GaussianNB  0.862083   0.758189  0.706449  0.705963  Glove_6B_300d\n",
       "1         BernoulliNB  0.866012   0.682701  0.648979  0.651998  Glove_6B_300d\n",
       "2        DecisionTree  0.808251   0.609814  0.606901  0.607175  Glove_6B_300d\n",
       "3          KNeighbors  0.915128   0.770038  0.779840  0.807331  Glove_6B_300d\n",
       "4       MLPClassifier  0.926523   0.819634  0.825720  0.846252  Glove_6B_300d\n",
       "5        RandomForest  0.917878   0.777735  0.796195  0.838551  Glove_6B_300d\n",
       "6          ExtraTrees  0.916306   0.774920  0.794646  0.835383  Glove_6B_300d\n",
       "7                 SVC  0.939489   0.842915  0.854771  0.878358  Glove_6B_300d\n",
       "8  LogisticRegression  0.936346   0.839541  0.851763  0.877172  Glove_6B_300d"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mean_models.head(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_models['Mean_score'] = df_mean_models.mean(axis=1)\n",
    "df_mean_models['Rank'] = df_mean_models['Mean_score'].rank(ascending=False)\n",
    "df_mean_models = df_mean_models.sort_values(by='Rank').reset_index(drop=True)\n",
    "df_mean_models = df_mean_models.loc[:, ['Embedding', 'Algorithm','Accuracy', 'Precision', 'F1_score', 'Recall', 'Mean_score', 'Rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_models.to_csv('models_mean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
